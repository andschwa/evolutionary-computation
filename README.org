#+TITLE:     Project #2 Genetic Program
#+AUTHOR:    Andrew Schwartzmeyer
#+EMAIL:     schw2620@vandals.uidaho.edu
#+DATE:      2014-03-26 Wed
#+LANGUAGE:  en
#+OPTIONS:   H:3 num:nil toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:t skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+INFOJS_OPT: view:nil toc:nil ltoc:t mouse:underline buttons:0 path:http://orgmode.org/org-info.js
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LATEX_HEADER: \usepackage{lmodern}

#+BEGIN_ABSTRACT
For the University of Idaho's Evolutionary Computation (CS 472)
Project #2 I implemented a Genetic Program in C++ to evolve a random
mathematical function fitted via symbolic regression to Terry Soule's
provided test data.  The function is modeled as a parse tree,
implemented recursively.  Algorithm details available in the table
below.  This implementation can achieve varying degrees of fitness
depending on the parameters it is run with, especially the maximum
number of iterations.  The code can be found at: =https://github.com/andschwa/uidaho-cs472-project2=
#+END_ABSTRACT

* Build :noexport:
Makes use of autotools. Necessary files:
- configure.ac (with help from autoscan)
- Makefile.am
- m4/* for macros

To configure and build:
#+begin_src sh
autoreconf -vfi && ./configure && make
#+end_src

Boost must be built using the same compiler, so for OS X,
=user-config.jam= needs the directive =using gcc : 4.8 : g++-4.8
;=. Boost should then be bootstrapped like thus:

#+begin_src sh
./bootstrap.sh --with-libraries=program_options --with-toolset=gcc
#+end_src

And then built with =./b2= and installed with =./b2 install=.

* Assignment :noexport:
** DONE Project #2a Genetic Program
   DEADLINE: <2014-03-07 Fri>
[[http://www2.cs.uidaho.edu/~cs472_572/s14/GPProjectA.html][From Professor Terry Soule]]
This is the first subproject of the GP project. The goal of this
subproject is to create a population of GP tree structures for a
symbolic regression problem.  If you want to use it, or refer to it, I
have written a node and an individual class that uses pointers to
build and evaluate random expression trees. Trees are build of nodes,
which point to each other.

node.h
node.cpp
individual.h
individual.cpp
test.cpp

To compile the test main program use:

=g++ test.cpp node.cpp individual.cpp=

For this subproject you only need the following functionallity:

- Generate full random expression individuals.
- The expression trees should have, at least, the non-teminals: +,
  -, *, /.
- The expression trees should have, at least, the teminals: X (the
  input variable) and constants.
- The ability to copy individuals.
- The ability to evaluate individuals.
- The ability to erase individuals.
- The ability to calculate the size (number of terminals and
  non-terminals) of individuals.
- The ability to create a population of individuals and to find the
  best and average fitness of the population, and the average size of
  the individuals in the population.
- Individuals should represent expression trees, but may be coded as a
  different type of data structure (e.g. a tree stored in an
  array). For now you may choose your own fitness function, i.e. your
  own set of x,y points that the GP should evolved an expression to
  fit.

For the report:

- Project Write-up: Write a short paper describing the results of your
  project that includes the following sections:
- Algorithm descriptions - Description of the GP so far. Be careful to
  include all of the details someone would need to replicate your
  work.
- Individual description - Description of the structure of your
  individuals. Be careful to include all of the details someone would
  need to replicate your work.
- Results - Basically, does it seem to be working.
- Conclusions - If it's not working, why not. And what are then next
  steps to complete the project.

** DONE Project #2b Genetic Program
   DEADLINE: <2014-03-14 Fri>
This is the second subproject of the GP project. The goal of this subproject is to finish the pieces of the GP for a symbolic regression problem.
For this subproject you will need to complete the GP including the following functionallity (in addition to the functions from the previous assignment):

- [X] Add a conditional to the function set of the expression trees.
- [X] Mutation
- [X] Crossover of two trees
- [X] Selection
- [X] Elitism if you are using a generational model
- [X] Test the GP to make sure that it is working.

Project Write-up: For this subproject you only need a description of
the general algorithm:

- [X] generational or steady-state
- [X] how mutation works
- [X] the selction mechanism, etc.
- [X] a description of any problems so far

Note that the write-up may be fairly short.
** DONE Project #2 Genetic Program
   DEADLINE: <2014-03-23 Sun>

This is the final part of Project 2. For this project you need to
present a summary of your GP program and the results. Here is a
template for the summary in Word and pdf (and the latex). Note that
for this project you do not need to do a lot of writting. An abstract,
fill in the table summarizing your algorithms, two graphs, and a
conclusion/discussion.

Given function:
[
if (x < -9)
y = 0.4 * ((20 + x)^{2} + 7 * x)
else if (x < 10)
y = 0.5 * x
else
 y = x + 5 * sin(0.5 * x)
] + random(5, -5)

* Algorithm Information
#+ATTR_LATEX: :align |l|p{4in}|
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Algorithm type   | Generational                                                                                                                                                 |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Population size  | 1024                                                                                                                                                         |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Selection method | Tournament of size 3                                                                                                                                         |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Elitism          | Replace random 2 offspring with previous best                                                                                                                |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Crossover method | Subtree with 90 percent chance to choose an internal node                                                                                                    |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Crossover rate   | 80 percent                                                                                                                                                   |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Mutation method  | Per node to same arity with 1 percent probability                                                                                                            |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Operation set    | sine, cosine, addition, subtraction, multiplication, protected division (return 1 if dividing by 0), lesser and greater than (with if/else logical branches) |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Terminal set     | input (x), constant (randomly drawn float from [0, 10])                                                                                                      |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Fitness function | Residual sum of squares with size penalty                                                                                                                    |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Size control     | Size penalty (0.5 * total nodes) in fitness function                                                                                                         |
|------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------|

** Changes Since Project #2b
The initial population generation is now created through the ramped
half-and-half method, that is, half of the trees are fully generated,
the other half are generated randomly.  The maximum depth of these
trees is randomly chosen from the interval [0, 3].  The lower initial
maximum depth was chosen due to code growth issues, as the trees soon
grow significantly larger.  The fitness function is the residual sum
of squares (of which the square-root is /not/ taken), plus the size
penalty.  Although fixing the maximum size to which trees can grow was
an option, the size penalty well enough (though far from perfect) and
was very easy to implement.  The penalty constant was chosen so that
the penalty produced a significant but not over-bearing (about 10
percent) addition to the fitness for the average size and fitness seen
in experimentation.  Additionally, multiple trials are run in
concurrent threads, rather than multiple runs of the program.

** Notes
The plotted data is the adjusted fitness, that is, =1/(1 + raw
fitness)= where raw fitness is the aforementioned residual sum of
squares plus size penalty.  This scales the fitness to the range [0,
1] where 1 is perfect (0 raw) and 0 is worst (infinite raw).  The size
penalty is subtracted from the fitness when adjusting it, so that it
does not skew the data.  Eight trials were run to 256 iterations, the
best two of which are reproduced here.

* Discussion

The graphs are to be found at the end of this report.

** Result #1

Elapsed time for this run: 479.597s

The best fitness of this trial climbed steadily upward until about
generation 175, where it started to plateau.  The best fitness has
dips, even with elitism, because of subtraction of the size penalty
from the adjusted fitness for the graphed data; this is interesting,
but I think to be expected, as the fitness penalty causes smaller,
though somewhat worse value-wise, functions to be considered better.
The average fitness slowly climbs, but plateaus rather quickly.  More
elitism may help in this case, or perhaps a higher mutation rate.

The function values of the found expression appear to be spot on in
the first region, as it captured the parabola quite
well. Additionally, it captured the large spike, which although it may
be noise, was significant.  The second region is less interesting,
because its noise swallows up its periodicity.  The third region is
less interesting still, but it was modeled pretty well by a straight
line.

** Result #2

Elapsed time for this run: 945.17s.

The best fitness for this result climbed very fast around the fourth
generation, and then continues to climb pretty steadily.  Again there
are infrequent drops where the algorithm chooses a similarly fit but
smaller function.  The average fitness plateaus a little less, but
still climbs slowly.

The function values for this result are a bit more interesting, and
really demonstrate the randomness of genetic programming.  Noise at
the bottom of the parabola in the first region was actually fit
opposite what it should be (that is, for the small valley, the
function has a hill, and vice versa for the small hill).  The noise in
the second region caused a slow wave to form.  The noisy but positive
slope in the third region was decently fit by the function.

* Conclusion

Overall this genetic program performed pretty well, and produced nice
results actually quite quickly, considering that multiple trials were
hogging the cores (which for a single trial are still utilized when
making a new offspring population).  There was a lot learned,
especially that the power function should probably not be included
(makes the average fitness generally go to infinity), and many bugs
squashed.  Further experimentation showed that more selection pressure
(tournament size of 4) seemed to produce better results in same number
of iterations, but took a about 50 generations to start producing any
results at all; however, this could have just been a quirk.  A
mutation rate of 4 percent (versus the earlier 1 percent) also seemed
to produce more fit functions, but tended to cause the over-fitting.

There are a few improvements I want to make: I want better control
over code-growth, with perhaps a size-limiting crossover, and I also
want to try no crossover but multiple types of randomly chosen
mutations (subtree, shrink, hoist, etc.).  All considered I will
probably spend more of this weekend working on these just for the fun
of it.

#+CAPTION: Best and average fitness for result #1
#+NAME: fig:fitness-1
[[./results_1/fitness.png]]

#+CAPTION: Evaluated function and test data for result #1
#+NAME: fig:function-1
[[./results_1/function.png]]

#+CAPTION: Best and average fitness for result #2
#+NAME: fig:fitness-2
[[./results_2/fitness.png]]

#+CAPTION: Evaluated function and test data for result #2
#+NAME: fig:function-1
[[./results_2/function.png]]
